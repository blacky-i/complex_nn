
@Article{Trufanov2021a,
  author    = {N N Trufanov and D V Churikov and O V Kravchenko},
  journal   = {Journal of Physics: Conference Series},
  title     = {Application of spectral analysis methods for data pre-processing of anomaly detection problem of vibration diagnostics in non-destructive testing},
  year      = {2021},
  month     = {nov},
  number    = {1},
  pages     = {012028},
  volume    = {2127},
  doi       = {10.1088/1742-6596/2127/1/012028},
  groups    = {Kravchenko, paper},
  publisher = {{IOP} Publishing},
}

@Article{Trabelsi2017,
  author        = {Chiheb Trabelsi and Olexa Bilaniuk and Ying Zhang and Dmitriy Serdyuk and Sandeep Subramanian and João Felipe Santos and Soroush Mehri and Negar Rostamzadeh and Yoshua Bengio and Christopher J Pal},
  title         = {Deep Complex Networks},
  year          = {2017},
  month         = may,
  abstract      = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.},
  archiveprefix = {arXiv},
  eprint        = {1705.09792},
  file          = {:Trabelsi2017 - Deep Complex Networks.pdf:PDF},
  groups        = {complex-valued, paper},
  keywords      = {cs.NE, cs.LG},
  primaryclass  = {cs.NE},
}

@Article{Kong2019,
  author        = {Qiuqiang Kong and Yin Cao and Turab Iqbal and Yuxuan Wang and Wenwu Wang and Mark D. Plumbley},
  title         = {PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition},
  year          = {2019},
  month         = dec,
  abstract      = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.},
  archiveprefix = {arXiv},
  eprint        = {1912.10211},
  file          = {:http\://arxiv.org/pdf/1912.10211v5:PDF},
  groups        = {audioset, paper},
  keywords      = {cs.SD, eess.AS},
  primaryclass  = {cs.SD},
  url           = {https://github.com/qiuqiangkong/audioset_tagging_cnn},
}

@Article{Fonseca2020,
  author        = {Eduardo Fonseca and Xavier Favory and Jordi Pons and Frederic Font and Xavier Serra},
  title         = {FSD50K: An Open Dataset of Human-Labeled Sound Events},
  year          = {2020},
  month         = oct,
  abstract      = {Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on over 2M tracks from YouTube videos and encompassing over 500 sound classes. However, AudioSet is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to YouTube videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.},
  archiveprefix = {arXiv},
  eprint        = {2010.00475},
  file          = {:Fonseca2020 - FSD50K_ an Open Dataset of Human Labeled Sound Events.pdf:PDF},
  groups        = {audioset, paper},
  keywords      = {cs.SD, cs.LG, eess.AS, stat.ML},
  primaryclass  = {cs.SD},
}

@Article{Koutini2021,
  author        = {Khaled Koutini and Jan Schlüter and Hamid Eghbal-zadeh and Gerhard Widmer},
  title         = {Efficient Training of Audio Transformers with Patchout},
  year          = {2021},
  month         = oct,
  abstract      = {The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST},
  archiveprefix = {arXiv},
  comment       = {Best mAP on FSD50k},
  eprint        = {2110.05069},
  file          = {:Koutini2021 - Efficient Training of Audio Transformers with Patchout.pdf:PDF},
  groups        = {audioset, paper},
  keywords      = {cs.SD, cs.LG, eess.AS},
  primaryclass  = {cs.SD},
}

@InProceedings{cances2018sound,
  author       = {Cances, L{\'e}o and Pellegrini, Thomas and Guyot, Patrice},
  booktitle    = {Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)-IEEE AASP 2018},
  title        = {Sound event detection from weak annotations: weighted-gru versus multi-instance-learning},
  year         = {2018},
  organization = {Tampere University of Technology},
  pages        = {64--68},
  groups       = {sound, paper},
}

@Article{kim2020occupant,
  author    = {Kim, Jinwoo and Min, Kyungjun and Jung, Minhyuk and Chi, Seokho},
  journal   = {Building and Environment},
  title     = {Occupant behavior monitoring and emergency event detection in single-person households using deep learning-based sound recognition},
  year      = {2020},
  pages     = {107092},
  volume    = {181},
  comment   = {healthcare},
  groups    = {sound, paper},
  publisher = {Elsevier},
}

@Article{khlaifi2018swallowing,
  author    = {Khlaifi, Hajer and Istrate, Dan and Demongeot, Jacques and Malouche, D},
  journal   = {IRBM},
  title     = {Swallowing sound recognition at home using GMM},
  year      = {2018},
  number    = {6},
  pages     = {407--412},
  volume    = {39},
  groups    = {sound, paper},
  publisher = {Elsevier},
}

@InProceedings{gemmeke2017audio,
  author       = {Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle    = {2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  title        = {Audio set: An ontology and human-labeled dataset for audio events},
  year         = {2017},
  organization = {IEEE},
  pages        = {776--780},
  groups       = {audioset, paper},
}

@InCollection{NEURIPS2019_9015,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  groups    = {Kravchenko, paper},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Article{guberman2016complex,
  author  = {Guberman, Nitzan},
  journal = {arXiv preprint arXiv:1602.09046},
  title   = {On complex valued convolutional neural networks},
  year    = {2016},
  groups  = {complex-valued, paper},
}

@InProceedings{NIPS2012_c399862d,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  groups    = {paper},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@Article{zeghidour2021wavesplit,
  author    = {Zeghidour, Neil and Grangier, David},
  journal   = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title     = {Wavesplit: End-to-end speech separation by speaker clustering},
  year      = {2021},
  pages     = {2840--2849},
  volume    = {29},
  groups    = {sound, paper},
  publisher = {IEEE},
}

@Article{subakan2022using,
  author  = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Grondin, Francois and Bronzi, Mirko},
  journal = {arXiv preprint arXiv:2202.02884},
  title   = {On Using Transformers for Speech-Separation},
  year    = {2022},
  groups  = {sound, paper},
}

@InProceedings{defossez2020real,
  author    = {Defossez, Alexandre and Synnaeve, Gabriel and Adi, Yossi},
  booktitle = {Interspeech},
  title     = {Real Time Speech Enhancement in the Waveform Domain},
  year      = {2020},
  groups    = {sound, paper},
}

@InProceedings{pandey2019tcnn,
  author       = {Pandey, Ashutosh and Wang, DeLiang},
  booktitle    = {ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title        = {TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain},
  year         = {2019},
  organization = {IEEE},
  pages        = {6875--6879},
  groups       = {sound, paper},
}

@InProceedings{xu2018large,
  author       = {Xu, Yong and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D},
  booktitle    = {2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  title        = {Large-scale weakly supervised audio classification using gated convolutional neural network},
  year         = {2018},
  organization = {IEEE},
  pages        = {121--125},
  groups       = {sound, paper},
}

@Article{zeng2019spectrogram,
  author    = {Zeng, Yuni and Mao, Hua and Peng, Dezhong and Yi, Zhang},
  journal   = {Multimedia Tools and Applications},
  title     = {Spectrogram based multi-task audio classification},
  year      = {2019},
  number    = {3},
  pages     = {3705--3722},
  volume    = {78},
  groups    = {sound, paper},
  publisher = {Springer},
}

@Article{leng2021fastcorrect,
  author  = {Leng, Yichong and Tan, Xu and Zhu, Linchen and Xu, Jin and Luo, Renqian and Liu, Linquan and Qin, Tao and Li, Xiangyang and Lin, Edward and Liu, Tie-Yan},
  journal = {Advances in Neural Information Processing Systems},
  title   = {FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition},
  year    = {2021},
  volume  = {34},
  groups  = {sound, paper},
}
